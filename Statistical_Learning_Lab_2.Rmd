---
title: "Statistical Learning and Data Analysis 2021 - 52525"
author: 'Abigail Gutman and Shahar Shalom '
date: "18/5/2021"
output:
  html_document: default
  pdf_document: default
subtitle: Lab 2 - Elections and RNA-sequencing
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
library('MASS')
library(data.table)
library(ggplot2)
library(kableExtra)
library(dplyr)
library(dendextend)
library(readxl)
library(fuzzyjoin)
library(stringr)
library(ggdendro)
library(factoextra)

options(scipen = 999)
```

# **1. Simulation Study:**

### Q1:

In this question we generate the first 10 coordinates of each µj vector j = 1, ..,3 : 
```{r, warning=FALSE}
ob <- c(rep(1,20),rep(2,30),rep(3,50)) 
sampling <- NULL

for (i in 1:10) {
first_10 <- rnorm(3,0,1) #mu
sampling <- rbind(sampling,first_10) 
colnames(sampling) <- c(1:3)
}
```


Here we creates an auxiliary functions that will help us in the following questions: 
```{r}
calculate.accuracy <- function(data, cluster_data){
  tab <- table(data,cluster_data)
  s <- 0
  b <- NULL
  for(i in 1:3){
    w<- tab[,i]
    m <- max(w)
    wm <- which.max(w)
    while(wm %in% b){
      w <- w[-wm,]
      m <- max(w)
      wm <- colnames(w)[which.max(w)]
    }
    s<- s+m
  }
  s <- s/100
  return(s)
  }


multi_fun <- function(s){
  start.time <- Sys.time()
  model <- kmeans(s,3,1,algorithm = "Lloyd")
  end.time <- Sys.time()
  time.taken <- end.time - start.time
  accurancy <- model$cluster
  accurancy <- calculate.accuracy(accurancy,ob)
  return(list(accurancy,time.taken))
}

```


### Q2:

Write a function that outputs a simulated dataset of dimension (100 × p)

```{r}
create_data <- function(sampling, p, sigma_e){
  x_data <- NULL
  if(p >10){
  mu_num <- rep(0,(p - 10))
  mu <- cbind(mu_num, mu_num, mu_num)
  mu <- rbind(sampling,mu)
  mu <- as.data.frame(mu) 
  } else {
    mu <- as.data.frame(sampling)
  }
  d <- diag(sqrt(rep(sigma_e,p)))
  dat_a = mvrnorm(20,mu = mu[,1],Sigma=  d)
  dat_b = mvrnorm(30,mu = mu[,2],Sigma=  d)
  dat_c = mvrnorm(50,mu = mu[,3],Sigma=  d)
  x_data = rbind(dat_a, dat_b,dat_c)
  return(x_data)
}
```


### Q3

Choose 4 levels of σ^2 and use p = 10, 20, 50.
choosing sigma : 1,2,6,9

```{r}
sigma_list <- c(1,2,6,9)
```


### Q4

For each combination of σ^2 and p we would like to generate multiple datasets (say B = 50).
We will write the solution for this question in the code of question 5 below. 

### Q5

For each data-set, run K-means once on the p dimensional data, and save accuracy and
run-time.

The following function performs fifty simulations for each combination of sigma and p. 
In which after the data is generated it calculates for it the accuracy of the K-MEAN algorithm and its runtime and keeps the information relevant to us for further reference only.

```{r, warning=FALSE}
p <- c(10,20,50)
simulation <- function(sampling,p,sigma_list,B){
  s_total <- NULL
  for (i in p) {
    for (j in sigma_list) {
      s <- NULL
      s <- replicate(n = B,sampling,simplify = F)
      s <- lapply(s,create_data,p = i,sigma_e = j)
      s <- lapply(s, multi_fun)
      s_total <- cbind(s_total,s)
    }
  }
    return(s_total)
}

gene <- simulation(sampling,p,sigma_list,50)
l <- c(paste0("p = ",10, ",sigma = ",1),paste0("p = ",10, ",sigma = ",2),
      paste0("p = ",10, ",sigma = ",6),paste0("p = ",10, ",sigma = ",9),
      paste0("p = ",20, ",sigma = ",1),paste0("p = ",20, ",sigma = ",2),
      paste0("p = ",20, ",sigma = ",6),paste0("p = ",20, ",sigma = ",9),
      paste0("p = ",50, ",sigma = ",1),paste0("p = ",50, ",sigma = ",2),
      paste0("p = ",50, ",sigma = ",6),paste0("p = ",50, ",sigma = ",9))

k_sh_p <- c(rep(10,4),rep(20,4),rep(50,4))
k_col_sig <- rep(sigma_list,3)
```


### Q6

Compute the average accuracy and the standard-error for each (p, σ2). Display these in a figure and a
table.

```{r,warning= False, message=FALSE}
temp <- NULL
temp2 <- NULL

for(i in 1:12){
  df <- gene[,i]
  df <- rbindlist(lapply(df, as.data.frame.list))
  colnames(df) <- c("accurancy", "run.time")
  temp2 <- rbind(temp2,list(df))
  df <- round(c(avg = mean(df$accurancy),sd_accurancy = sd(df$accurancy)/sqrt(50)),3)
  temp <- rbind(temp,df)
}
names(temp2) <- l
row.names(temp) <- l
```

```{r}
temp <- as.data.frame(temp)
temp <- cbind(temp,k_col_sig,k_sh_p)
```

```{r}
temp <- as.data.frame(temp)
ggplot(temp,aes( y = avg,x=factor(k_col_sig))) +
  geom_bar(stat="identity",position="dodge") +
  facet_grid(~factor(k_sh_p))+
  geom_errorbar(data = temp,aes(ymin=avg-sd_accurancy,
  ymax=avg + sd_accurancy), width=.2,position=position_dodge(.9)) + 
    geom_text(aes(label=avg), vjust=1.6, color="white", size=3.5)+
  facet_grid(~factor(k_sh_p))+ theme_light()  +
  xlab("Devided by sigma & p levels")+
  ylab("Accurancy Average") +
  ggtitle(label = "Averege acurrency bar plot", subtitle = "Caculated over 50 simulations. Displayed by the different sigmas and p with standard error bar") +
  theme(plot.title = element_text(hjust = 0.5,size = 20, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5,size = 10, face = "bold")) +
  theme(legend.title = element_text(color = "black", size = 10),
          legend.text = element_text(color = "black")) +
  labs(shape="Vectors Variables \n Number", colour="SE of \n the Vectors") + ylim(c(0,1))
```



The graph shows the average accuracy for fifty simulations for each combination of sigma and p compared to perfect accuracy. For each column you can see the standard deviation of the accuracy for the simulations. It can be understood from the low standard steak that the accuracy compliance by the number of simulations is relatively reliable. The graph shows the difference in the ability to achieve high accuracy in classification by KMEAN as the standard deviation of the distribution from it is sampled, and the more variables there are.



The data shown in the table:
```{r}

kable(temp[,1:2]) %>% kable_styling()
```
`
```{r}
temp2 <-bind_rows(temp2, .id = "column_label")
k_sh_p <- c(rep(10,4*50),rep(20,4*50),rep(50,4*50))
k_col_sig <- rep(rep(sigma_list, each = 50),3)
temp2 <- cbind(temp2,k_col_sig,k_sh_p)
ggplot(temp2, aes(y = run.time, x =factor(k_col_sig))) + facet_grid(~factor(k_sh_p))+
 geom_boxplot()+
  stat_summary(geom = "point",
    fun = "mean", col = "red",size = 1, fill = "red") +
    theme_light() + ylim(c(0,0.002))+
  xlab("")+ ylab("Run time for K-mean convergence") +
  ggtitle(label = "Run time", subtitle = "Displayed by diffrenet sigmas and variable amouמt. \n Mean marked in red point5, Median marked in bold line. \n outliers have been left out.")+
  theme(plot.title = element_text(hjust = 0.5,size = 20, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5,size = 10, face = "bold")) +
  theme(axis.text.x = element_text(angle = 00, vjust = 0.5)) 

```

###Q8
We have seen that the SE we obtained on the average accuracy of the simulations allows us to treat the average accuracy as reliable. On this basis it can be seen that the larger the standard deviation of the distribution from which we have drawn in advance the greater we encounter the lower ability of KMEAN to perform a correct classification, regardless of the length of the vector and the amount of variables in it. In addition it can be seen that the more variables we have in each vector, here too the algorithm has difficulty performing a correct classification, regardless of the standard deviation. It can also be seen in the graph of the runtime required for the algorithm convergence that there is an increase in time for large standard deviations and multivariate vectors, except for outliers, it should be noted that in this case these are relatively low dimensions, but for Big Data the effect may be significant.


# **2. Comparing demographic and election data:**

```{r, message=F, error=F, warning=F}
#load the data frames
knesset_df <- read.csv("C:/Users/Shahar/Dropbox/zugi/lemida/knesset_24.csv", encoding = "UTF-8")
CBS_df <- read_excel("C:/Users/Shahar/Dropbox/zugi/lemida/t01.xls")
```

### Q1

In this question we sampled 20 different locations that we would like to investigate.
We used the socio-economic data from 2013 from the ISB website for the socio-economic data frame and the 24 election results data frame.

```{r,message=F}
#first, we merge the data frames to find the matching locations in both. 
merged_df <- left_join(CBS_df, knesset_df, by = "סמל.ישוב")

#add "שם.ישוב" coloumn to the CBS data frame 
names_df <- as.data.frame(knesset_df[,2:3])
CBS_df <- left_join(CBS_df, names_df)

#semple 20 locations 
set.seed(20)
Semp_20 <- sample(merged_df$"סמל.ישוב" , 20)

# sum all the votes for each location 
#calculate the persents of votes for each party in every location 
knesset_df_norm <- knesset_df %>%rowwise %>% mutate(total_votes = sum(c_across( 8:46))) %>% ungroup() %>%  mutate(round(across(8:46, ~ . / total_votes),3))

#filter each data frame for the sempled values.
knesset_semp20 <- filter(knesset_df_norm, knesset_df_norm$"סמל.ישוב" %in% Semp_20)
CBS_semp20 <- filter(CBS_df, CBS_df$"סמל.ישוב" %in% Semp_20) 
```


### Q2
In this question we construct a hierarchical tree for the elections data.

for this calculation we decided to take only the scaled votes results from the data frame, the reason is that we want to investigate wich cities behave in a similer way and to see if the algorithem will collapse this cites to similer clusters. 

We decided to define the distances between two cities by the complete linked algorithem. we want to see in the tree the conection between clusters of cities that have the maximal distance between them. 

```{r}
knesst_dist <- knesset_semp20[,c(8:46)] %>% dist %>% hclust(method = "complete")
knesst_dend <- as.dendrogram(knesst_dist)
labels(knesst_dend) <- as.character(knesset_semp20$'שם.ישוב')

Knesst_plot <- ggplot(knesst_dend %>% 
                        set('branches_lwd', 0.7) %>%
                        set('labels_cex', 0.8),
                        horiz = TRUE,  theme = theme_minimal()) + 
                    ggtitle("Hirarchical tree of the 24 election datas with complete linked algorithm", "*Each color present diffrent cluster") + ylab("Distance") + xlab(" ")

Knesst_plot
```


### Q3
In this question we construct a hierarchical tree for the socio-economic 2013 data frame.
we used only the values which dascribe the diffrenete in socio-economic status between the cities.

Again we decided to use the complete linked algorithm.
```{r}
CBS_dist <- CBS_semp20[,6:17] %>% scale %>% dist %>% hclust(method = "complete")
CBS_dend <- as.dendrogram(CBS_dist)
labels(CBS_dend) <- as.character(CBS_semp20$'שם.ישוב')

CBS_plot <- ggplot(CBS_dend %>% 
                     set('branches_lwd', 0.7) %>% 
                     set('labels_cex', 0.8), horiz = TRUE, theme = theme_minimal()) + 
                  ggtitle("Hirarchical tree of the CBS datas with complete linked algorithm", "*Each color present diffrent cluster")+ ylab("Distance") + xlab(" ")

CBS_plot
```


### Q4

In this question we tried to visualized the diffrents between the two dendograms.

Even before we see the visual comparison we tend to think that while there are some cities that have received a similar characterization in the two dendograms, yet most cities get a different height and sometimes completely opposite from the height they received in the other dendogram.

```{r}
dl <- dendlist(highlight_branches_col(knesst_dend), highlight_branches_col(CBS_dend))

tanglegram(dl, sort = TRUE, highlight_distinct_edges  = FALSE, highlight_branches_lwd = FALSE, main_left = "CBS", main_right = "Knesset", dLeaf_left = 0.01 , dLeaf_right = -0.6) 
```

After examining the visual comparison between the trees we understand that our hypothesis is Almost right< most of the cities were given a different location in each tree and there appears to be no connection between the two trees 

However, this does not mean that one of the dendograms is incorrect, the reason for the large differences between the two is a different effect of socioeconomic status on the population in each city, in addition we should remember that the population in Israel is very far from being a homogeneous population. 


### Q5

In this question we were asked to choos score wich calculate the similarity between the two trees. 
We choose the bakers gamma score, it is defined as the rank correlation between the stages at which pairs of objects combine in each of the two trees.


```{r, message=F, error=F, warning=F}
dend <- knesst_dend
dend1 <- CBS_dend

cor <- cor_bakers_gamma(dend,dend1)
print(paste("The correlation between the two dendograms is: ", round(cor,3)))
```

In this case we see that we got small correlation between the two dendograms. 
it means that thr two hirarchical trees almost dosent have conection between them.
the conection is a small nagative conection. 


### Q6

In this question we were asked to find a background distribution for this score. 
To do this we randomly permute the labels of the knesset tree while keeping the labels of thr CBS tree fixed. 
Our null hypothesis is that the correlation between the two trees is that the dendogram are fully correlated to herself in all of the permutations that we create. 


```{r, message=F, error=F, warning=F}
the_cor <- cor_bakers_gamma(dend,dend) #we see that the correlation between the dend to herself is 1
the_cor2 <- cor_bakers_gamma(dend,dend1) #the correlation between the two dendograms that we created

R <- 100
cor_bakers_gamma_results <- numeric(R)
dend_mixed <- dend
for(i in 1:R) { #compute the corr between the original dendograme to the permutation dendogram 
   dend_mixed <- sample.dendrogram(dend_mixed, replace = FALSE)
   cor_bakers_gamma_results[i] <- cor_bakers_gamma(dend, dend_mixed)
}

plot(density(cor_bakers_gamma_results),
     main = "Baker's gamma distribution under H0",
     xlim = c(-1,1))
abline(v = 0, lty = 2)
abline(v = the_cor, lty = 2, col = 2)
abline(v = the_cor2, lty = 2, col = 4)
legend("topleft", legend = c("cor", "cor2"), fill = c(2,4))
round(sum(the_cor2 < cor_bakers_gamma_results)/ R, 4)
title(sub = paste("One sided p-value:",
                  "cor =",  round(sum(the_cor < cor_bakers_gamma_results)/ R, 4),
                  " ; cor2 =",  round(sum(the_cor2 < cor_bakers_gamma_results)/ R, 4)))
```

In this graph we see the background distribution for the correlation score.
we see that cor = 1, represent the correlation between the knesset dendogram to herself, and test the H0 that all of the premuted dendograms are fully correlated to the original dendogram. 
we can see in the graph that this isent what happend. 
Cor2 = -0.04, we check the P-value of this result and see that Pv >0.05, theres for we reject H0 and say that the premuted dendogram is not fully correalted to the original dendogram. 

We have learned that althogh we thoght that permutation dendogram will give us similear trees from the same data frames it doesn't happend and we get total new dendograme in each permutation. 


# **3. Exploratory analysis of RNA seq data:**

```{r, message=F, error=F, warning=F}

k_means_shiny <- function(k){
  data_prep <- m
  m_new <- as.data.frame(data_prep[sample(1:53,k),])
  clus <- apply(data_prep,1 ,FUN = c_fun, m=m_new)
  m_old <- as.data.frame(data_prep[sample(1:53,k),])
  data_prep <- cbind(data_prep, clus)
  iter <- 1
  while(any(abs(m_new - m_old)) > 0.1 & iter < 1){
  data_prep <- as.data.frame(data_prep)
  m_old <- m_new
  m_new <- aggregate(. ~ clus, data_prep, FUN = mean)
  m_new <- m_new[,-1]
  data_prep$clus<- data_prep[,-1]  %>% apply(1,FUN = c_fun,m = m_new)
  print(m_new)
  print(m_old)
  iter <- iter +1

  }
  return(data_prep)
}

c_fun <- function(d,m){
  p <- NULL
  for (i in 1:length(m[,1])) {
    x <- rbind(d,m[i,])
    di <- dist(x, method = 'euclidean')
    p <- cbind(p,di)
  }
  p <- which.min(p)
  return(p)
}

g <- k_means_shiny(6)
```

```{r}

med_dat <- read.delim('C:/Users/abig4/OneDrive/Documents/GitHub/Statistical-Learning_Lab_2/gtex_Kmeans/gtex.gct',
                      skip = 2 ,row.names=c(1) , header = TRUE)
#med_dat <- read.delim("C:/Users/Shahar/Documents/GitHub/Statistical-Learning_Lab_2/gtex_Kmeans/gtex.gct",
#                      skip = 2 ,row.names=c(1) , header = TRUE)
gen_names <- med_dat[, 1]
med_dat <- med_dat[,-1]
ti_name <- colnames(med_dat)
med_dat <- transpose(med_dat)
med_dat <-med_dat[,colMeans(med_dat) > 0]
v <- apply(med_dat,2, var)
med_dat <-med_dat[, v != 0]
#med_dat <- as.data.frame(scale(med_dat,T,T))
m <- prcomp(med_dat,scale. = T,center = T)
sv <-m$sdev^2/sum(m$sdev^2)
sv <- cumsum(sv)
sv <- length(sv[sv < 0.9])
m <- m$x[,1:sv]

```


```{r}


```



