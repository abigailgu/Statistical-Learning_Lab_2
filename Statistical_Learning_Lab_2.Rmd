---
title: "Statistical Learning and Data Analysis 2021 - 52525"
author: 'Abigail Gutman and Shahar Shalom '
date: "18/5/2021"
output:
  html_document: default
  pdf_document: default
subtitle: Lab 2 - Elections and RNA-sequencing
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
library('MASS')
library(data.table)
library(ggplot2)
library(kableExtra)
library(dplyr)
library(dendextend)
library(readxl)
library(fuzzyjoin)
library(stringr)
library(ggdendro)
library(factoextra)
```

# **1. Simulation Study:**

### Q1:
Generate the first 10 coordinates of each µj vector j = 1, ..,3
```{r, warning=FALSE}
options(scipen = 999)


ob <- cbind(rep(1,20),rep(2,30),rep(3,50))
sampling <- NULL
for (i in 1:10) {
first_10 <- rnorm(3,0,1) #mu
sampling <- rbind(sampling,first_10)
colnames(sampling) <- c(1:3)
}


```

```{r}
calculate.accuracy <- function(data, cluster_data)
{
  matching <- Map(function(data, cluster_data) { data %in% cluster_data }, data, cluster_data)
  tf <- unlist(matching, use.names=FALSE)
  return (sum(tf)/length(tf))
}
```


```{r}

multi_fun <- function(s){
  start.time <- Sys.time()
  model <- kmeans(s,3,1,algorithm = "Lloyd")
  end.time <- Sys.time()
  time.taken <- end.time - start.time
  accurancy <- model$cluster
  accurancy <- calculate.accuracy(accurancy,ob)
  return(list(accurancy,time.taken))
}

```

### Q2:
Write a function that outputs a simulated dataset of dimension (100 × p)
```{r}
create_data <- function(sampling, p, sigma_e){
  x_data <- NULL
  if(p >10){
  mu_num <- rep(0,(p - 10))
  mu <- cbind(mu_num, mu_num, mu_num)
  mu <- rbind(sampling,mu)
  mu <- as.data.frame(mu) 
  } else {
    mu <- as.data.frame(sampling)
  }
  d <- diag(sqrt(rep(sigma_e,p)))
  dat_a = mvrnorm(20,mu = mu[,1],Sigma=  d)
  dat_b = mvrnorm(30,mu = mu[,2],Sigma=  d)
  dat_c = mvrnorm(50,mu = mu[,3],Sigma=  d)
  x_data = rbind(dat_a, dat_b,dat_c)
  return(x_data)
}
```

### Q3
Choose 4 levels of σ^2 and use p = 10, 20, 50.
choosing sigma : 1,7,150,212

### Q4
For each combination of σ^2 and p, generate multiple datasets (say B = 50).

### Q5
For each data-set, run K-means once on the p dimensional data, and save accuracy and
run-time.
```{r, warning=FALSE}
sigma_list <- c(1,7,150,212)
p <- c(10,20,50)
simulation <- function(sampling,p,sigma_list,B){
  s_total <- NULL
  for (i in p) {
    for (j in sigma_list) {
      s <- NULL
      s <- replicate( n = B,sampling,simplify = F)
      s <- lapply(s,create_data,p = i,sigma_e = j)
      s <- lapply(s, multi_fun)
      s_total <- cbind(s_total,s)
    }
  }
    return(s_total)
}

gene <- simulation(sampling,p,sigma_list,50)
l <- c(paste0("p = ",10, ",sigma = ",1),paste0("p = ",10, ",sigma = ",7),
      paste0("p = ",10, ",sigma = ",150),paste0("p = ",10, ",sigma = ",212),
      paste0("p = ",20, ",sigma = ",1),paste0("p = ",20, ",sigma = ",7),
      paste0("p = ",20, ",sigma = ",150),paste0("p = ",20, ",sigma = ",212),
      paste0("p = ",50, ",sigma = ",1),paste0("p = ",50, ",sigma = ",7),
      paste0("p = ",50, ",sigma = ",150),paste0("p = ",50, ",sigma = ",212))
```


### Q6
Compute the average accuracy and the standard-error for each (p, σ2). Display these in a figure and a
table.

```{r,warning= False, message=FALSE}
temp <- NULL
temp2 <- NULL

for(i in 1:12){
  df <- gene[,i]
  df <- rbindlist(lapply(df, as.data.frame.list))
  colnames(df) <- c("accurancy", "run.time")
  temp2 <- rbind(temp2,list(df))
  df <- round(c(avg = mean(df$accurancy),sd_accurancy = sd(df$accurancy)),3)
  temp <- rbind(temp,df)
}
names(temp2) <- l
row.names(temp) <- l
```


```{r}
temp <- as.data.frame(temp)
ggplot(temp,aes(x = sd_accurancy, y = avg)) + geom_point()

```

```{r}
#kable(temp) %>% kable_styling()
```

```{r}
temp2 <-bind_rows(temp2, .id = "column_label")
ggplot(temp2, aes(x = run.time, y = accurancy, group = column_label, col = column_label)) + geom_point() + facet_wrap(~column_label)+ theme(legend.position = "none")
```




# **2. Comparing demographic and election data:**

```{r, message=F, error=F, warning=F}
#load the data frames
knesset_df <- read.csv("C:/Users/Shahar/Dropbox/zugi/lemida/knesset_24.csv", encoding = "UTF-8")
CBS_df <- read_excel("C:/Users/Shahar/Dropbox/zugi/lemida/t01.xls")
```

### Q1

In this question we sampled 20 different locations that we would like to investigate.
We used the socio economic data from 2013 from the ISB website for the socio economic data and the 24 election results data.

```{r,message=F}
#first, we merge the data frames to find the matching locations in both. 
merged_df <- left_join(CBS_df, knesset_df, by = "סמל.ישוב")

#add "שם.ישוב" coloumn to the CBS data frame 
names_df <- as.data.frame(knesset_df[,2:3])
CBS_df <- left_join(CBS_df, names_df)

#semple 20 locations 
Semp_20 <- sample(merged_df$"סמל.ישוב" , 20)

# sum all the votes for each location 
#calculate the persents of votes for each party in every location 
knesset_df_norm <- knesset_df %>%rowwise %>% mutate(total_votes = sum(c_across( 8:46))) %>% ungroup() %>%  mutate(round(across(8:46, ~ . / total_votes),3))

#filter each data frame for the sempled values.
knesset_semp20 <- filter(knesset_df_norm, knesset_df_norm$"סמל.ישוב" %in% Semp_20)
CBS_semp20 <- filter(CBS_df, CBS_df$"סמל.ישוב" %in% Semp_20) 
```


### Q2

In this question we created dendogram that present the minimum distance between the 20 locations that we sampled and divides them into four clusters.
we used the Single linked method becouse we would like to see clusters of cities that  the voting patterns among them are the closest.


#לנסות לבדוק מה מספר האשכולות האופטימלי
#לנסות לבדוק את השיטה של החלוקה ולהראות שמספק האשכולות מוצדק 
```{r}
knesst_dist <- knesset_semp20[,c(8:46)] %>% dist %>% hclust(method = "complete")
knesst_dend <- as.dendrogram(knesst_dist)
labels(knesst_dend) <- as.character(knesset_semp20$'שם.ישוב')

Knesst_plot <- ggplot(knesst_dend %>% set('branches_k_color', k = 4) %>%
                set('branches_lwd', 0.7) %>%
                set('labels_colors', k = 4) %>%
                set('labels_cex', 0.8), 
                horiz = TRUE) + ggtitle("Hirarchical tree of the 24 election datas with Single linked algorithm", "*Each color present diffrent cluster")

Knesst_plot
```


### Q3
Similer to Q2 we choose here to use the singel linked method so we can see in each cluster the cities that have similar Characteristic properties which used for calculating a socio-economic index. 

```{r}
CBS_dist <- CBS_semp20[,6:17] %>% scale %>% dist %>% hclust(method = "single")
CBS_dend <- as.dendrogram(CBS_dist)
labels(CBS_dend) <- as.character(CBS_semp20$'שם.ישוב')

CBS_plot <- ggplot(CBS_dend %>% set('branches_k_color', k = 4) %>%
                set('branches_lwd', 0.7) %>%
                set('labels_colors', k = 4) %>%
                set('labels_cex', 0.8), 
                horiz = TRUE) + ggtitle("Hirarchical tree of the CBS datas with Single linked algorithm", "*Each color present diffrent cluster")

CBS_plot
```


### Q4

In this question we tried to visualized the diffrents between the two dendograms.

Even before we see the visual comparison we tend to think that while there are some cities that have received a similar characterization in the two dendograms, yet most cities get a different height and sometimes completely opposite from the height they received in the other dendogram.

```{r}
dl <- dendlist(highlight_branches_col(knesst_dend), highlight_branches_col(CBS_dend))

tanglegram(dl, sort = TRUE, common_subtrees_color_lines = FALSE, highlight_distinct_edges  = FALSE, highlight_branches_lwd = FALSE, main_left = "CBS", main_right = "Knesset") 
```

After examining the visual comparison between the trees we understand that our hypothesis is correct and that although both trees have some exact characteristics but most cities get a completely different characterization in each of the dendograms.

However, this does not mean that one of the dendograms is incorrect, the reason for the large differences between the two is a different effect of socioeconomic status on the population in each city, in addition we should remember that the population in the State of Israel is very far from being a homogeneous population. 


### Q5
```{r, message=F, error=F, warning=F}

```


### Q6
```{r, message=F, error=F, warning=F}

```



# **3. Exploratory analysis of RNA seq data:**

```{r, message=F, error=F, warning=F}
library(dplyr)
library(data.table)
k_means_shiny <- function(k){
  data_prep <- scale(med_dat,T,T)
  data_prep[is.na(data_prep)] <- 0
  m_new <- sample_n(med_dat,k)
  c_t <- apply(data_prep,1 ,FUN = c_fun, m=m_new)
  
  
  #while(any(abs(m_new - m_old)) > 0.001){
  #data_prep["cluster"] <- c_t
  #m_old <- m_new
  #m_new <- data_prep %>% group_by(cluster) %>% summarise(mean)
  #c_t <- data_prep[,!"cluster"]  %>% lapply(FUN = c_fun,m = m_new)
  #print(data_prep[,"cluster"])
  #}
  #return(data_prep[,"cluster"])
}

c_fun <- function(d,m){
  p <- (sweep(m,2,d))**2
  print(m)
  print(p)
  p <- apply(p,1,sum)
  p <- which.min(p)
  return(p)
}

med_dat <- read.delim('C:/Users/abig4/OneDrive/Documents/GitHub/Statistical-Learning_Lab_2/gtex_Kmeans/gtex.gct',
                      skip = 2 ,row.names=c(1) , header = TRUE)
gen_names <- med_dat[, 1]
med_dat <- med_dat[,-1]
med_dat <- transpose(med_dat)



k_means_shiny(15)

```