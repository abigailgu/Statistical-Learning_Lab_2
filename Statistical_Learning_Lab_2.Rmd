---
title: "Statistical Learning and Data Analysis 2021 - 52525"
author: 'Abigail Gutman and Shahar Shalom '
date: "18/5/2021"
output:
  html_document: default
  pdf_document: default
subtitle: Lab 2 - Elections and RNA-sequencing
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
library('MASS')
library(data.table)
library(ggplot2)
library(kableExtra)
library(dplyr)
library(dendextend)
library(readxl)
library(fuzzyjoin)
library(stringr)
library(ggdendro)
library(factoextra)

options(scipen = 999)
```

# **1. Simulation Study:**

### Q1:

In this question we generate the first 10 coordinates of each µj vector j = 1, ..,3 : 
```{r, warning=FALSE}
ob <- c(rep(1,20),rep(2,30),rep(3,50)) 
sampling <- NULL

for (i in 1:10) {
first_10 <- rnorm(3,0,1) #mu
sampling <- rbind(sampling,first_10) 
colnames(sampling) <- c(1:3)
}
```


Here we creates an auxiliary functions that will help us in the following questions: 
```{r}
calculate.accuracy <- function(data, cluster_data){
  tab <- table(data,cluster_data)
  s <- 0
  b <- NULL
  for(i in 1:3){
    w<- tab[,i]
    m <- max(w)
    wm <- which.max(w)
    while(wm %in% b){
      w <- w[-wm,]
      m <- max(w)
      wm <- colnames(w)[which.max(w)]
    }
    s<- s+m
  }
  s <- s/100
  return(s)
  }


multi_fun <- function(s){
  start.time <- Sys.time()
  model <- kmeans(s,3,1,algorithm = "Lloyd")
  end.time <- Sys.time()
  time.taken <- end.time - start.time
  accurancy <- model$cluster
  accurancy <- calculate.accuracy(accurancy,ob)
  return(list(accurancy,time.taken))
}

```


### Q2:

Write a function that outputs a simulated dataset of dimension (100 × p)

```{r}
create_data <- function(sampling, p, sigma_e){
  x_data <- NULL
  if(p >10){
  mu_num <- rep(0,(p - 10))
  mu <- cbind(mu_num, mu_num, mu_num)
  mu <- rbind(sampling,mu)
  mu <- as.data.frame(mu) 
  } else {
    mu <- as.data.frame(sampling)
  }
  d <- diag(sqrt(rep(sigma_e,p)))
  dat_a = mvrnorm(20,mu = mu[,1],Sigma=  d)
  dat_b = mvrnorm(30,mu = mu[,2],Sigma=  d)
  dat_c = mvrnorm(50,mu = mu[,3],Sigma=  d)
  x_data = rbind(dat_a, dat_b,dat_c)
  return(x_data)
}
```


### Q3

Choose 4 levels of σ^2 and use p = 10, 20, 50.
choosing sigma : 1,2,6,9

```{r}
sigma_list <- c(1,2,6,9)
```


### Q4

For each combination of σ^2 and p we would like to generate multiple datasets (say B = 50).
We will write the solution for this question in the code of question 5 below. 

### Q5

For each data-set, run K-means once on the p dimensional data, and save accuracy and
run-time.

The following function performs fifty simulations for each combination of sigma and p. 
In which after the data is generated it calculates for it the accuracy of the K-MEAN algorithm and its runtime and keeps the information relevant to us for further reference only.

```{r, warning=FALSE}
p <- c(10,20,50)
simulation <- function(sampling,p,sigma_list,B){
  s_total <- NULL
  for (i in p) {
    for (j in sigma_list) {
      s <- NULL
      s <- replicate(n = B,sampling,simplify = F)
      s <- lapply(s,create_data,p = i,sigma_e = j)
      s <- lapply(s, multi_fun)
      s_total <- cbind(s_total,s)
    }
  }
    return(s_total)
}

gene <- simulation(sampling,p,sigma_list,50)
l <- c(paste0("p = ",10, ",sigma = ",1),paste0("p = ",10, ",sigma = ",2),
      paste0("p = ",10, ",sigma = ",6),paste0("p = ",10, ",sigma = ",9),
      paste0("p = ",20, ",sigma = ",1),paste0("p = ",20, ",sigma = ",2),
      paste0("p = ",20, ",sigma = ",6),paste0("p = ",20, ",sigma = ",9),
      paste0("p = ",50, ",sigma = ",1),paste0("p = ",50, ",sigma = ",2),
      paste0("p = ",50, ",sigma = ",6),paste0("p = ",50, ",sigma = ",9))

k_sh_p <- c(rep(10,4),rep(20,4),rep(50,4))
k_col_sig <- rep(sigma_list,3)
```


### Q6

Compute the average accuracy and the standard-error for each (p, σ2). Display these in a figure and a
table.

```{r,warning= False, message=FALSE}
temp <- NULL
temp2 <- NULL

for(i in 1:12){
  df <- gene[,i]
  df <- rbindlist(lapply(df, as.data.frame.list))
  colnames(df) <- c("accurancy", "run.time")
  temp2 <- rbind(temp2,list(df))
  df <- round(c(avg = mean(df$accurancy),sd_accurancy = sd(df$accurancy)/sqrt(50)),3)
  temp <- rbind(temp,df)
}
names(temp2) <- l
row.names(temp) <- l
```

```{r}
temp <- as.data.frame(temp)
temp <- cbind(temp,k_col_sig,k_sh_p)
```

```{r}
temp <- as.data.frame(temp)
ggplot(temp,aes( y = avg,x=factor(k_col_sig))) +
  geom_bar(stat="identity",position="dodge") +
  facet_grid(~factor(k_sh_p))+
  geom_errorbar(data = temp,aes(ymin=avg-sd_accurancy,
  ymax=avg + sd_accurancy), width=.2,position=position_dodge(.9)) + 
    geom_text(aes(label=avg), vjust=1.6, color="white", size=3.5)+
  facet_grid(~factor(k_sh_p))+ theme_light()  +
  xlab("Devided by p level")+
  ylab("Accurancy Average") +
  ggtitle(label = "Averege acurrency bar plot", subtitle = "Caculated over 50 simulations. Displayed by the different sigmas and p with standard error bar") +
  theme(plot.title = element_text(hjust = 0.5,size = 20, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5,size = 10, face = "bold")) +
  theme(legend.title = element_text(color = "black", size = 10),
          legend.text = element_text(color = "black")) +
  labs(shape="Vectors Variables \n Number", colour="SE of \n the Vectors") + ylim(c(0,1))
```


```{r}

#kable(temp) %>% kable_styling()
```


```{r}
temp2 <-bind_rows(temp2, .id = "column_label")
k_sh_p <- c(rep(10,4*50),rep(20,4*50),rep(50,4*50))
k_col_sig <- rep(rep(sigma_list, each = 50),3)
temp2 <- cbind(temp2,k_col_sig,k_sh_p)

ggplot(temp2, aes(x = run.time, y = accurancy, color =factor(k_sh_p) )) +
  geom_point(position = position_jitter(w = 0.001))+ facet_wrap(~factor(k_col_sig))+ theme(legend.position = "none") +
    theme_gray()  + ylim(c(0,1)) +
  ylab("Accurancy of K-means")+ xlab("Run time for K-mean convergence") +
  ggtitle(label = "Run time vs Accurancy", subtitle = "Displayed diffrant sigmas and p.") +   theme(plot.title = element_text(hjust = 0.5,size = 20, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5,size = 10, face = "bold"))
```
```{r}
ggplot(temp2, aes(y = run.time, x =factor(k_col_sig))) + facet_grid(~factor(k_sh_p))+
  geom_point(size = 1,alpha = 0.6) + 
  stat_summary(geom = "point",
    fun = "mean", col = "red",size = 1, fill = "red") +
  theme(legend.position = "none") + 
    theme_light() + ylim(c(0,0.002))+
  xlab("")+ ylab("Run time for K-mean convergence") +
  ggtitle(label = "Run time vs Accurancy", subtitle = "Displayed diffrant sigmas and p.") +
  theme(plot.title = element_text(hjust = 0.5,size = 20, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5,size = 10, face = "bold")) +
  theme(axis.text.x = element_text(angle = 00, vjust = 0.5))

```

# **2. Comparing demographic and election data:**

```{r, message=F, error=F, warning=F}
#load the data frames
knesset_df <- read.csv("C:/Users/Shahar/Dropbox/zugi/lemida/knesset_24.csv", encoding = "UTF-8")
CBS_df <- read_excel("C:/Users/Shahar/Dropbox/zugi/lemida/t01.xls")
```

### Q1

In this question we sampled 20 different locations that we would like to investigate.
We used the socio-economic data from 2013 from the ISB website for the socio-economic data frame and the 24 election results data frame.

```{r,message=F}
#first, we merge the data frames to find the matching locations in both. 
merged_df <- left_join(CBS_df, knesset_df, by = "סמל.ישוב")

#add "שם.ישוב" coloumn to the CBS data frame 
names_df <- as.data.frame(knesset_df[,2:3])
CBS_df <- left_join(CBS_df, names_df)

#semple 20 locations 
set.seed(20)
Semp_20 <- sample(merged_df$"סמל.ישוב" , 20)

# sum all the votes for each location 
#calculate the persents of votes for each party in every location 
knesset_df_norm <- knesset_df %>%rowwise %>% mutate(total_votes = sum(c_across( 8:46))) %>% ungroup() %>%  mutate(round(across(8:46, ~ . / total_votes),3))

#filter each data frame for the sempled values.
knesset_semp20 <- filter(knesset_df_norm, knesset_df_norm$"סמל.ישוב" %in% Semp_20)
CBS_semp20 <- filter(CBS_df, CBS_df$"סמל.ישוב" %in% Semp_20) 
```


### Q2
In this question we construct a hierarchical tree for the elections data.

for this calculation we decided to take only the scaled votes results from the data frame, the reason is that we want to investigate wich cities behave in a similer way and to see if the algorithem will collapse this cites to similer clusters. 

We decided to define the distances between two cities by the complete linked algorithem. we want to see in the tree the conection between clusters of cities that have the maximal distance between them. 

```{r}
knesst_dist <- knesset_semp20[,c(8:46)] %>% dist %>% hclust(method = "complete")
knesst_dend <- as.dendrogram(knesst_dist)
labels(knesst_dend) <- as.character(knesset_semp20$'שם.ישוב')

Knesst_plot <- ggplot(knesst_dend %>% 
                        set('branches_lwd', 0.7) %>%
                        set('labels_cex', 0.8),
                        horiz = TRUE,  theme = theme_minimal()) + 
                    ggtitle("Hirarchical tree of the 24 election datas with complete linked algorithm", "*Each color present diffrent cluster") + ylab("Distance") + xlab(" ")

Knesst_plot
```


### Q3
In this question we construct a hierarchical tree for the socio-economic 2013 data frame.
we used only the values which dascribe the diffrenete in socio-economic status between the cities.

Again we decided to use the complete linked algorithm.
```{r}
CBS_dist <- CBS_semp20[,6:17] %>% scale %>% dist %>% hclust(method = "complete")
CBS_dend <- as.dendrogram(CBS_dist)
labels(CBS_dend) <- as.character(CBS_semp20$'שם.ישוב')

CBS_plot <- ggplot(CBS_dend %>% 
                     set('branches_lwd', 0.7) %>% 
                     set('labels_cex', 0.8), horiz = TRUE, theme = theme_minimal()) + 
                  ggtitle("Hirarchical tree of the CBS datas with complete linked algorithm", "*Each color present diffrent cluster")+ ylab("Distance") + xlab(" ")

CBS_plot
```


### Q4

In this question we tried to visualized the diffrents between the two dendograms.

Even before we see the visual comparison we tend to think that while there are some cities that have received a similar characterization in the two dendograms, yet most cities get a different height and sometimes completely opposite from the height they received in the other dendogram.

```{r}
dl <- dendlist(highlight_branches_col(knesst_dend), highlight_branches_col(CBS_dend))

tanglegram(dl, sort = TRUE, highlight_distinct_edges  = FALSE, highlight_branches_lwd = FALSE, main_left = "CBS", main_right = "Knesset", dLeaf_left = 0.01 , dLeaf_right = -0.6) 
```

After examining the visual comparison between the trees we understand that our hypothesis is Almost right< most of the cities were given a different location in each tree and there appears to be no connection between the two trees 

However, this does not mean that one of the dendograms is incorrect, the reason for the large differences between the two is a different effect of socioeconomic status on the population in each city, in addition we should remember that the population in Israel is very far from being a homogeneous population. 


### Q5

In this question we were asked to choos score wich calculate the similarity between the two trees. 
We choose the bakers gamma score, it is defined as the rank correlation between the stages at which pairs of objects combine in each of the two trees.


```{r, message=F, error=F, warning=F}
dend <- knesst_dend
dend1 <- CBS_dend

cor <- cor_bakers_gamma(dend,dend1)
print(paste("The correlation between the two dendograms is: ", round(cor,3)))
```

In this case we see that we got small correlation between the two dendograms. 
it means that thr two hirarchical trees almost dosent have conection between them.
the conection is a small nagative conection. 


### Q6

In this question we were asked to find a background distribution for this score. 

השערת האפס הינה שהקורלציה בין הפרמוטציות גבוהה יותר מהקורלציה שקיבלנו בשני העצים 


```{r, message=F, error=F, warning=F}
the_cor <- cor_bakers_gamma(dend,dend)
the_cor2 <- cor_bakers_gamma(dend,dend1)

R <- 100
cor_bakers_gamma_results <- numeric(R)
dend_mixed <- dend
for(i in 1:R) {
   dend_mixed <- sample.dendrogram(dend_mixed, replace = FALSE)
   cor_bakers_gamma_results[i] <- cor_bakers_gamma(dend, dend_mixed)
}

plot(density(cor_bakers_gamma_results),
     main = "Baker's gamma distribution under H0",
     xlim = c(-1,1))
abline(v = 0, lty = 2)
abline(v = the_cor, lty = 2, col = 2)
abline(v = the_cor2, lty = 2, col = 4)
legend("topleft", legend = c("cor", "cor2"), fill = c(2,4))
round(sum(the_cor2 < cor_bakers_gamma_results)/ R, 4)
title(sub = paste("One sided p-value:",
                  "cor =",  round(sum(the_cor < cor_bakers_gamma_results)/ R, 4),
                  " ; cor2 =",  round(sum(the_cor2 < cor_bakers_gamma_results)/ R, 4)))

```



# **3. Exploratory analysis of RNA seq data:**

```{r, message=F, error=F, warning=F}
set.seed(42)
k_means_shiny <- function(k){
  data_prep <- scale(med_dat_2,T,T)
  data_prep[is.na(data_prep)] <- 0
  
  m_new <- as.data.frame(data_prep[sample(1:53,k),])
  clus <- apply(data_prep,1 ,FUN = c_fun, m=m_new)
  m_old <- as.data.frame(data_prep[sample(1:53,k),])
  data_prep <- cbind(data_prep, clus)
  
  while(any(abs(m_new - m_old)) > 0.1){
  data_prep <- as.data.frame(data_prep)
  m_old <- m_new
  m_new <- aggregate(.~clus, data_prep, FUN = mean)
  m_new <- m_new[,-1]
  data_prep$clus<- data_prep[,-1]  %>% apply(1,FUN = c_fun,m = m_new)
  }
  return(data_prep[clus])
}

c_fun <- function(d,m){
  p <- sweep(m,2,d,FUN = "-")
  p <- (as.data.frame(p))^2
  p <- sqrt(as.matrix(apply(p,1, sum)))
  p <- which.min(p)
  return(p)
}

k_means_shiny(15)
```


```{r}

med_dat <- read.delim('C:/Users/abig4/OneDrive/Documents/GitHub/Statistical-Learning_Lab_2/gtex_Kmeans/gtex.gct',
                      skip = 2 ,row.names=c(1) , header = TRUE)
#med_dat <- read.delim("C:/Users/Shahar/Documents/GitHub/Statistical-Learning_Lab_2/gtex_Kmeans/gtex.gct",
#                      skip = 2 ,row.names=c(1) , header = TRUE)
gen_names <- med_dat[, 1]
med_dat <- med_dat[,-1]
med_dat <- transpose(med_dat)
med_dat_2 <- as.data.frame(med_dat[,1:100])

```